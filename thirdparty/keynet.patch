diff --git a/keyNet/model/keynet_architecture.py b/keyNet/model/keynet_architecture.py
index c181a01..7c17296 100644
--- a/keyNet/model/keynet_architecture.py
+++ b/keyNet/model/keynet_architecture.py
@@ -1,7 +1,31 @@
 import math
 import numpy as np
-import tensorflow as tf
-
+if False:
+    import tensorflow as tf
+    import tensorflow.contrib as tf_contrib
+else:
+    # from https://stackoverflow.com/questions/56820327/the-name-tf-session-is-deprecated-please-use-tf-compat-v1-session-instead
+    import tensorflow.compat.v1 as tf
+    import tensorflow as tfv2
+    #import tensorflow.contrib as tf_contrib
+    from tensorflow.keras.layers import Layer, Lambda, BatchNormalization
+
+
+class SafeBatchNorm(Layer):
+    def __init__(self, name=None):
+        super().__init__(name=name)
+        self.bn = None
+
+    def build(self, input_shape):
+        if self.bn is None:
+            self.bn = BatchNormalization(scale=True, name=self.name + "_bn")
+            self.bn.build(input_shape)
+        super().build(input_shape)
+
+    def call(self, inputs, training=False):
+        return self.bn(inputs, training=training)
+    
+    
 def gaussian_multiple_channels(num_channels, sigma):
 
     r = 2*sigma
@@ -113,7 +137,7 @@ class keynet(object):
         tf.set_random_seed(args.random_seed)
         np.random.seed(args.random_seed)
 
-        name_scope = tf.contrib.framework.get_name_scope()
+        name_scope = tfv2.get_current_name_scope()
 
         # Smooth Gausian Filter
         gaussian_avg = gaussian_multiple_channels(1, 1.5)
@@ -167,8 +191,19 @@ class keynet(object):
 
         features, network = self.compute_features(input_data, dim, reuse, is_training)
 
-        features = tf.layers.batch_normalization(inputs=features, scale=True, training=is_training,
-                                                 name=tf.contrib.framework.get_name_scope() + '_batch_final', reuse=reuse)
+        # features = tf.layers.batch_normalization(inputs=features, scale=True, training=is_training,
+        #                                          name=tfv2.get_current_name_scope() + '_batch_final', reuse=reuse)
+        if not hasattr(self, 'final_bn'):
+            self.final_bn = tf.keras.layers.BatchNormalization(
+                scale=True,
+                name=tfv2.get_current_name_scope() + '_batch_final'
+            )    
+            
+        features = tf.cond(
+            is_training,
+            lambda: self.final_bn(features, training=True),
+            lambda: self.final_bn(features, training=False)
+        )                
 
         output = self.conv_block(features, 'last_layer', reuse, is_training, num_filters=1, size_kernel=self.conv_kernel_size, batchnorm=False, activation_function=False)
 
@@ -245,7 +280,7 @@ class keynet(object):
             input_data_resized = self.local_norm_image(input_data_resized)
 
             features_t, network = self.compute_handcrafted_features(input_data_resized, network, idx_level,
-                                                                    tf.contrib.framework.get_name_scope())
+                                                                    tfv2.get_current_name_scope())
 
             for idx_layer in range(self.num_blocks):
                 features_t = self.conv_block(features_t, str(idx_layer + 1), reuse or idx_level > 0, is_training,
@@ -263,18 +298,54 @@ class keynet(object):
 
     def conv_block(self, features, name, reuse, is_training, num_filters, size_kernel, batchnorm=True, activation_function=True):
 
-        features = tf.layers.conv2d(inputs=features, filters=num_filters,
-                                    kernel_size=size_kernel,
-                                    strides=1, padding='SAME', use_bias=True,
-                                    kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),
-                                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
-                                    data_format='channels_last',
-                                    name=tf.contrib.framework.get_name_scope() + '_conv_'+name, reuse=reuse)
+        # features = tf.layers.conv2d(inputs=features, filters=num_filters,
+        #                             kernel_size=size_kernel,
+        #                             strides=1, padding='SAME', use_bias=True,
+        #                             kernel_initializer=tfv2.initializers.variance_scaling(),
+        #                             kernel_regularizer=tfv2.keras.regularizers.L2(0.1),
+        #                             data_format='channels_last',
+        #                             name=tfv2.get_current_name_scope() + '_conv_'+name, reuse=reuse)
+        
+        conv_layer_name = tfv2.get_current_name_scope() + '_conv_' + name
+        if not hasattr(self, '_conv_layers'): # Create a dict for conv layers if they are named
+            self._conv_layers = {}        
+        if conv_layer_name not in self._conv_layers:
+            self._conv_layers[conv_layer_name] = tf.keras.layers.Conv2D(
+                filters=num_filters,
+                kernel_size=size_kernel,
+                strides=1,
+                padding='same',
+                use_bias=True,
+                kernel_initializer=tf.keras.initializers.VarianceScaling(),
+                kernel_regularizer=tf.keras.regularizers.L2(0.1),
+                data_format='channels_last',
+                name=conv_layer_name
+            )
+        features = self._conv_layers[conv_layer_name](features) 
 
         if batchnorm:
-            features = tf.layers.batch_normalization(inputs=features, scale=True, training=is_training,
-                                                 name=tf.contrib.framework.get_name_scope() + '_batch_'+name, reuse=reuse)
+            # features = tf.layers.batch_normalization(inputs=features, scale=True, training=is_training,
+            #                                      name=tfv2.get_current_name_scope() + '_batch_'+name, reuse=reuse)
+            bn_name = tfv2.get_current_name_scope() + '_batch_' + name
+            if not hasattr(self, 'bn_layers'):
+                self.bn_layers = {}
+
+            if bn_name not in self.bn_layers:
+                bn_layer = tf.keras.layers.BatchNormalization(scale=True, name=bn_name)
+
+                # Manually build using known shape
+                input_shape = tf.TensorShape([None, None, None, num_filters])
+                bn_layer.build(input_shape)
+
+                self.bn_layers[bn_name] = bn_layer
+
+            features = tf.cond(
+                is_training,
+                lambda: self.bn_layers[bn_name](features, training=True),
+                lambda: self.bn_layers[bn_name](features, training=False)
+            )
 
+        
         if activation_function:
             features = tf.nn.relu(features)
 
